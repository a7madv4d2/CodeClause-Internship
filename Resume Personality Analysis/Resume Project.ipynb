{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa29c1af-c886-4056-9896-653dfb3e0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import os \n",
    "from openai import OpenAI\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2961c2-8cf6-4012-a2a2-d0bb089e9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install streamlit pypdf python-docx\n",
    "#pip install google-generativeai\n",
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad23a23-eff9-45bb-a9fa-0a1bb161be72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7062157-20bd-48ed-9493-2b2d613c7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE_PATH = 'UpdatedResumeDataSet.csv'\n",
    "OUTPUT_FILE = 'labeled_resume_data.json'\n",
    "CHECKPOINT_FREQUENCY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "839b7b89-7d79-42dd-8989-f9bf5065ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - selecting the provider\n",
    "API_PROVIDER = 'openai'\n",
    "\n",
    "API_KEY = 'sk-4408764883724ccfa264c0b471b25cff'\n",
    "BASE_URL = 'https://api.deepseek.com'\n",
    "MODEL_NAME = \"deepseek-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28082374-13ec-4a8f-bce9-b2c335b98be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = '''\n",
    "You are an expert Psychometrician and HR Data Scientist.\n",
    "Your job is to analyze resume text and infer the canditate's Big Five Personality Traits.\n",
    "You must ouput your answer in strict JSON format.\n",
    "Do not include any conversational text.\n",
    "Output format:\n",
    "{\n",
    "    \"openness\": <float 0.0-1.0>,\n",
    "    \"conscientiousness\": <float 0.0-1.0>,\n",
    "    \"extroversion\": <float 0.0-1.0>,\n",
    "    \"agreeableness\": <float 0.0-1.0>,\n",
    "    \"neuroticism\": <float 0.0-1.0>\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85548fa6-e741-4f29-8dcc-9e8a6eb5832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealLLMLabeler:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key= API_KEY, base_url=BASE_URL)\n",
    "\n",
    "    def analyze_resume(self, resume_text):\n",
    "        \"\"\"\n",
    "        Sends the text to the LLM and parses the JSON response.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model = MODEL_NAME, \n",
    "                messages = [\n",
    "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'content': f\"Analyze this resume:\\n\\n{resume_text[:4000]}\"}\n",
    "                ],\n",
    "                response_format = {'type': 'json_object'},\n",
    "                temperature = 0.1 # low temp = more deterministic/consistent analysis\n",
    "            )\n",
    "            # Extract the text string from the response\n",
    "            content = response.choices[0].message.content\n",
    "            # Parse the string into a py dictionary\n",
    "            return json.load(content)\n",
    "        except Exception as e:\n",
    "            print(f\"API errpr: {e}\")\n",
    "            # Return neutral scores if API fails\n",
    "            return {\"openness\": 0.5, \"conscientiousness\": 0.5, \"extroversion\": 0.5, \"agreeableness\": 0.5, \"neuroticism\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a48d1fe-8064-4f9a-b9b3-114718588299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.labler = RealLLMLabeler()\n",
    "\n",
    "    def process(self):\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            # sample only 5 rows, to avoid burning the api credits\n",
    "            df = df.head(5)\n",
    "            print(f\"Loaded {len(df)}\")\n",
    "        except FileNotFoundError:\n",
    "            print('CSV not found.')\n",
    "            return\n",
    "        labeled_dataset = []\n",
    "    \n",
    "        # Loop through resumes\n",
    "        for index, row in df.iterrows():\n",
    "            text = row.get('Resume', '') \n",
    "            print(f\"Processing Resume #{index}...\")\n",
    "    \n",
    "            # The real API call\n",
    "            traits = self.labeler.analyze_resume(text)\n",
    "            point(f\"Dervied traits: {traits}\")\n",
    "    \n",
    "            # Calculate Meta Features (Structural)\n",
    "            meta = [len(text), text.count('‚Ä¢'), 0.1]\n",
    "            \n",
    "            labeled_dataset.append({\n",
    "                \"id\": index,\n",
    "                \"text\": text,\n",
    "                \"meta_features\": meta,\n",
    "                \"labels\": list(traits.values())\n",
    "            })\n",
    "            \n",
    "            # Sleep to avoid hitting Rate Limits\n",
    "            time.sleep(1)\n",
    "        # Save\n",
    "        with open('real_labeled_data.json', 'w') as f:\n",
    "            json.dump(labeled_dataset, f, indent= 4)\n",
    "        print(\" Done! 'real_labeled_data.json' is ready for the RL Agent.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = DataPipeline('UpdatedResumeDataset.csv')\n",
    "    pipeline.process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25b1aa68-03b2-4aa9-9bf6-f607340d4a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Heuristic Labeling (Free Mode)...\n",
      "üìÑ Processing 962 resumes...\n",
      "‚úÖ Success! Generated 962 labeled samples.\n",
      "üìÇ Saved to: final_labeled_dataset.json\n",
      "üëâ You can now proceed to train the RL Agent.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_CSV = 'UpdatedResumeDataSet.csv'\n",
    "OUTPUT_FILE = 'final_labeled_dataset.json'\n",
    "\n",
    "class HeuristicLabeler:\n",
    "    def __init__(self):\n",
    "        # Psychological Keyword Dictionaries\n",
    "        # These words strongly correlate with specific Big 5 traits\n",
    "        self.keywords = {\n",
    "            \"openness\": [\"creative\", \"design\", \"art\", \"innovative\", \"research\", \"novel\", \"concept\", \"graphic\"],\n",
    "            \"conscientiousness\": [\"organized\", \"managed\", \"delivered\", \"deadline\", \"plan\", \"budget\", \"schedule\", \"efficient\"],\n",
    "            \"extroversion\": [\"team\", \"leadership\", \"presented\", \"spoke\", \"communication\", \"sales\", \"client\", \"negotiated\"],\n",
    "            \"agreeableness\": [\"collaborated\", \"support\", \"helped\", \"assist\", \"volunteered\", \"care\", \"community\", \"mentor\"],\n",
    "            \"neuroticism\": [\"critical\", \"issue\", \"problem\", \"stress\", \"pressure\", \"urgent\", \"fix\", \"error\"]\n",
    "        }\n",
    "\n",
    "    def score_trait(self, text, trait):\n",
    "        \"\"\"\n",
    "        Calculates a score (0.0 - 1.0) based on keyword density.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        word_count = len(text.split())\n",
    "        if word_count == 0: return 0.5\n",
    "        \n",
    "        matches = 0\n",
    "        for word in self.keywords[trait]:\n",
    "            matches += len(re.findall(r'\\b' + word + r'\\b', text))\n",
    "        \n",
    "        # Logic: More keywords = Higher Score\n",
    "        # We normalize it so it doesn't exceed 1.0\n",
    "        # Base score is 0.3, max added is 0.7\n",
    "        score = 0.3 + (matches * 0.15) \n",
    "        \n",
    "        # Add some random noise to make it realistic (humans aren't perfect)\n",
    "        noise = random.uniform(-0.05, 0.05)\n",
    "        return min(max(score + noise, 0.1), 0.95)\n",
    "\n",
    "    def process(self):\n",
    "        print(f\"üöÄ Starting Heuristic Labeling (Free Mode)...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(INPUT_CSV)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Error: '{INPUT_CSV}' not found.\")\n",
    "            return\n",
    "\n",
    "        labeled_data = []\n",
    "\n",
    "        # Find the text column\n",
    "        text_col = None\n",
    "        for col in df.columns:\n",
    "            if col in ['Resume', 'Resume_str', 'text', 'content']:\n",
    "                text_col = col\n",
    "                break\n",
    "        \n",
    "        if not text_col:\n",
    "            print(\"‚ùå Could not find Resume column.\")\n",
    "            return\n",
    "\n",
    "        print(f\"üìÑ Processing {len(df)} resumes...\")\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            text = str(row[text_col])\n",
    "            \n",
    "            # 1. Calculate Scores using Heuristics\n",
    "            traits = {\n",
    "                \"openness\": round(self.score_trait(text, \"openness\"), 2),\n",
    "                \"conscientiousness\": round(self.score_trait(text, \"conscientiousness\"), 2),\n",
    "                \"extroversion\": round(self.score_trait(text, \"extroversion\"), 2),\n",
    "                \"agreeableness\": round(self.score_trait(text, \"agreeableness\"), 2),\n",
    "                \"neuroticism\": round(self.score_trait(text, \"neuroticism\"), 2),\n",
    "            }\n",
    "            \n",
    "            # 2. Calculate Meta Features (Structure)\n",
    "            meta = [len(text), text.count('‚Ä¢'), 0.1]\n",
    "\n",
    "            labeled_data.append({\n",
    "                \"id\": index,\n",
    "                \"text\": text,\n",
    "                \"meta_features\": meta,\n",
    "                \"labels\": list(traits.values())\n",
    "            })\n",
    "\n",
    "        # 3. Save\n",
    "        with open(OUTPUT_FILE, 'w') as f:\n",
    "            json.dump(labeled_data, f, indent=4)\n",
    "            \n",
    "        print(f\"‚úÖ Success! Generated {len(labeled_data)} labeled samples.\")\n",
    "        print(f\"üìÇ Saved to: {OUTPUT_FILE}\")\n",
    "        print(\"üëâ You can now proceed to train the RL Agent.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    labeler = HeuristicLabeler()\n",
    "    labeler.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf009e9e-2c56-4438-98b7-8399ac486a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Gemini Free Tier Labeling...\n",
      "üîÑ Resuming from 962...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_CSV = 'UpdatedResumeDataSet.csv'\n",
    "OUTPUT_FILE = 'final_labeled_dataset.json'\n",
    "CHECKPOINT_FREQUENCY = 20 # Save often\n",
    "\n",
    "# --- GET FREE KEY AT: https://aistudio.google.com/app/apikey ---\n",
    "GOOGLE_API_KEY = \"AIzaSyBIna2HOvlgxEVY6SzfVae4iTYbjSvuJnw\"\n",
    "\n",
    "class GeminiPipeline:\n",
    "    def __init__(self):\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        # Use the Flash model (Fast & Free Tier available)\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        \n",
    "    def find_resume_column(self, df):\n",
    "        possible_names = ['Resume', 'Resume_str', 'text', 'content', 'resume_text', 'cv']\n",
    "        for col in df.columns:\n",
    "            if col in possible_names:\n",
    "                return col\n",
    "        return None\n",
    "\n",
    "    def analyze_resume(self, text):\n",
    "        prompt = f\"\"\"\n",
    "        Act as a Psychometrician. Analyze this resume text and estimate the Big Five Personality traits.\n",
    "        Return ONLY a JSON object. No markdown, no text explanation.\n",
    "        Format: {{\"openness\": 0.0, \"conscientiousness\": 0.0, \"extroversion\": 0.0, \"agreeableness\": 0.0, \"neuroticism\": 0.0}}\n",
    "        \n",
    "        Resume Text:\n",
    "        {text[:4000]}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Generate content\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            # Extract text\n",
    "            raw_text = response.text\n",
    "            \n",
    "            # Clean up potential markdown formatting (```json ... ```)\n",
    "            clean_text = raw_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "            return json.loads(clean_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Frequent error if you hit the rate limit (15 req/min)\n",
    "            if \"429\" in str(e):\n",
    "                print(\"‚è≥ Hit Free Tier Rate Limit. Sleeping for 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run(self):\n",
    "        if not os.path.exists(INPUT_CSV):\n",
    "            print(\"‚ùå CSV not found.\")\n",
    "            return\n",
    "\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "        text_col = self.find_resume_column(df)\n",
    "        \n",
    "        if not text_col:\n",
    "            print(\"‚ùå No Resume column found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ Starting Gemini Free Tier Labeling...\")\n",
    "        \n",
    "        processed_data = []\n",
    "        if os.path.exists(OUTPUT_FILE):\n",
    "            try:\n",
    "                with open(OUTPUT_FILE, 'r') as f:\n",
    "                    processed_data = json.load(f)\n",
    "                    print(f\"üîÑ Resuming from {len(processed_data)}...\")\n",
    "            except: pass\n",
    "\n",
    "        remaining_df = df.iloc[len(processed_data):]\n",
    "\n",
    "        for index, row in tqdm(remaining_df.iterrows(), total=len(remaining_df)):\n",
    "            resume_text = str(row[text_col])\n",
    "            \n",
    "            if len(resume_text) < 10: continue\n",
    "\n",
    "            traits = self.analyze_resume(resume_text)\n",
    "            \n",
    "            if traits:\n",
    "                meta = [len(resume_text), resume_text.count('‚Ä¢'), 0.1]\n",
    "                processed_data.append({\n",
    "                    \"id\": index,\n",
    "                    \"text\": resume_text,\n",
    "                    \"meta_features\": meta,\n",
    "                    \"labels\": list(traits.values())\n",
    "                })\n",
    "            \n",
    "            # CHECKPOINT\n",
    "            if len(processed_data) % CHECKPOINT_FREQUENCY == 0:\n",
    "                with open(OUTPUT_FILE, 'w') as f:\n",
    "                    json.dump(processed_data, f, indent=4)\n",
    "            \n",
    "            # CRITICAL: SLEEP TO STAY IN FREE TIER\n",
    "            # Limit is usually 15 RPM (Requests Per Minute)\n",
    "            # So we sleep 4 seconds between calls (60s / 15 = 4s)\n",
    "            time.sleep(4)\n",
    "\n",
    "        with open(OUTPUT_FILE, 'w') as f:\n",
    "            json.dump(processed_data, f, indent=4)\n",
    "        print(\"‚úÖ Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = GeminiPipeline()\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdbe2d8e-84c8-4036-b98e-42890c030e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'final_labeled_dataset.json'\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "EPISODES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6149aaa4-d1f3-4684-8008-c3cffa8140e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Model\n",
      "Loaded 962 labeled resumes for training\n",
      "Starting AI Training\n",
      "Episode 1: Loss=0.0411\n",
      "Episode 1: Loss=0.0922\n",
      "Episode 1: Loss=0.1144\n",
      "Episode 1: Loss=0.1469\n",
      "Episode 1: Loss=0.1845\n",
      "Episode 1: Loss=0.2225\n",
      "Episode 1: Loss=0.2393\n",
      "Episode 1: Loss=0.2933\n",
      "Episode 1: Loss=0.3660\n",
      "Episode 1: Loss=0.4056\n",
      "Episode 1: Loss=0.4480\n",
      "Episode 1: Loss=0.4657\n",
      "Episode 1: Loss=0.4904\n",
      "Episode 1: Loss=0.5020\n",
      "Episode 1: Loss=0.5370\n",
      "Episode 1: Loss=0.5644\n",
      "Episode 1: Loss=0.6702\n",
      "Episode 1: Loss=0.7150\n",
      "Episode 1: Loss=0.7268\n",
      "Episode 1: Loss=0.7743\n",
      "Episode 1: Loss=0.7914\n",
      "Episode 1: Loss=0.8682\n",
      "Episode 1: Loss=0.8848\n",
      "Episode 1: Loss=0.9006\n",
      "Episode 1: Loss=0.9709\n",
      "Episode 1: Loss=1.1503\n",
      "Episode 1: Loss=1.1888\n",
      "Episode 1: Loss=1.2866\n",
      "Episode 1: Loss=1.3149\n",
      "Episode 1: Loss=1.3504\n",
      "Episode 1: Loss=1.3627\n",
      "Episode 1: Loss=1.3831\n",
      "Episode 1: Loss=1.4928\n",
      "Episode 1: Loss=1.5280\n",
      "Episode 1: Loss=1.6534\n",
      "Episode 1: Loss=1.6609\n",
      "Episode 1: Loss=1.6922\n",
      "Episode 1: Loss=1.7225\n",
      "Episode 1: Loss=1.8253\n",
      "Episode 1: Loss=1.8407\n",
      "Episode 1: Loss=1.8672\n",
      "Episode 1: Loss=1.8765\n",
      "Episode 1: Loss=1.9220\n",
      "Episode 1: Loss=2.0411\n",
      "Episode 1: Loss=2.0617\n",
      "Episode 1: Loss=2.0788\n",
      "Episode 1: Loss=2.1045\n",
      "Episode 1: Loss=2.1358\n",
      "Episode 1: Loss=2.1970\n",
      "Episode 1: Loss=2.2556\n",
      "Episode 2: Loss=0.1133\n",
      "Episode 2: Loss=0.1353\n",
      "Episode 2: Loss=0.1864\n",
      "Episode 2: Loss=0.2263\n",
      "Episode 2: Loss=0.2590\n",
      "Episode 2: Loss=0.2843\n",
      "Episode 2: Loss=0.2963\n",
      "Episode 2: Loss=0.3234\n",
      "Episode 2: Loss=0.3528\n",
      "Episode 2: Loss=0.3993\n",
      "Episode 2: Loss=0.4356\n",
      "Episode 2: Loss=0.4626\n",
      "Episode 2: Loss=0.4687\n",
      "Episode 2: Loss=0.5073\n",
      "Episode 2: Loss=0.5386\n",
      "Episode 2: Loss=0.5444\n",
      "Episode 2: Loss=0.5746\n",
      "Episode 2: Loss=0.6036\n",
      "Episode 2: Loss=0.6484\n",
      "Episode 2: Loss=0.6754\n",
      "Episode 2: Loss=0.6920\n",
      "Episode 2: Loss=0.7627\n",
      "Episode 2: Loss=0.7894\n",
      "Episode 2: Loss=0.8157\n",
      "Episode 2: Loss=0.8212\n",
      "Episode 2: Loss=0.8476\n",
      "Episode 2: Loss=0.8955\n",
      "Episode 2: Loss=1.0501\n",
      "Episode 2: Loss=1.3381\n",
      "Episode 2: Loss=1.3806\n",
      "Episode 2: Loss=1.4021\n",
      "Episode 2: Loss=1.4533\n",
      "Episode 2: Loss=1.6258\n",
      "Episode 2: Loss=1.6684\n",
      "Episode 2: Loss=1.7204\n",
      "Episode 2: Loss=1.7570\n",
      "Episode 2: Loss=1.8262\n",
      "Episode 2: Loss=1.9435\n",
      "Episode 2: Loss=1.9787\n",
      "Episode 2: Loss=2.0034\n",
      "Episode 2: Loss=2.0695\n",
      "Episode 2: Loss=2.2137\n",
      "Episode 2: Loss=2.2394\n",
      "Episode 2: Loss=2.3969\n",
      "Episode 2: Loss=2.4518\n",
      "Episode 2: Loss=2.5084\n",
      "Episode 2: Loss=2.5670\n",
      "Episode 2: Loss=2.6037\n",
      "Episode 2: Loss=2.6386\n",
      "Episode 2: Loss=2.6899\n",
      "Episode 3: Loss=0.0294\n",
      "Episode 3: Loss=0.1290\n",
      "Episode 3: Loss=0.1816\n",
      "Episode 3: Loss=0.2238\n",
      "Episode 3: Loss=0.2501\n",
      "Episode 3: Loss=0.2977\n",
      "Episode 3: Loss=0.3178\n",
      "Episode 3: Loss=0.3627\n",
      "Episode 3: Loss=0.3821\n",
      "Episode 3: Loss=0.4114\n",
      "Episode 3: Loss=0.4433\n",
      "Episode 3: Loss=0.4760\n",
      "Episode 3: Loss=0.5025\n",
      "Episode 3: Loss=0.5349\n",
      "Episode 3: Loss=0.5579\n",
      "Episode 3: Loss=0.5795\n",
      "Episode 3: Loss=0.7098\n",
      "Episode 3: Loss=0.7331\n",
      "Episode 3: Loss=0.9426\n",
      "Episode 3: Loss=0.9641\n",
      "Episode 3: Loss=0.9860\n",
      "Episode 3: Loss=1.0009\n",
      "Episode 3: Loss=1.0230\n",
      "Episode 3: Loss=1.1761\n",
      "Episode 3: Loss=1.1979\n",
      "Episode 3: Loss=1.2334\n",
      "Episode 3: Loss=1.2451\n",
      "Episode 3: Loss=1.2689\n",
      "Episode 3: Loss=1.3101\n",
      "Episode 3: Loss=1.3284\n",
      "Episode 3: Loss=1.3556\n",
      "Episode 3: Loss=1.3688\n",
      "Episode 3: Loss=1.4801\n",
      "Episode 3: Loss=1.5428\n",
      "Episode 3: Loss=1.5916\n",
      "Episode 3: Loss=1.6062\n",
      "Episode 3: Loss=1.6215\n",
      "Episode 3: Loss=1.6606\n",
      "Episode 3: Loss=1.6791\n",
      "Episode 3: Loss=1.7002\n",
      "Episode 3: Loss=1.7136\n",
      "Episode 3: Loss=1.7299\n",
      "Episode 3: Loss=1.7392\n",
      "Episode 3: Loss=1.7559\n",
      "Episode 3: Loss=1.8016\n",
      "Episode 3: Loss=1.8137\n",
      "Episode 3: Loss=1.8184\n",
      "Episode 3: Loss=1.9248\n",
      "Episode 3: Loss=1.9592\n",
      "Episode 3: Loss=1.9650\n",
      "Episode 4: Loss=0.0327\n",
      "Episode 4: Loss=0.0738\n",
      "Episode 4: Loss=0.0907\n",
      "Episode 4: Loss=0.1548\n",
      "Episode 4: Loss=0.1692\n",
      "Episode 4: Loss=0.2063\n",
      "Episode 4: Loss=0.2166\n",
      "Episode 4: Loss=0.2275\n",
      "Episode 4: Loss=0.2316\n",
      "Episode 4: Loss=0.2690\n",
      "Episode 4: Loss=0.3780\n",
      "Episode 4: Loss=0.4972\n",
      "Episode 4: Loss=0.5030\n",
      "Episode 4: Loss=0.5830\n",
      "Episode 4: Loss=0.6006\n",
      "Episode 4: Loss=0.6203\n",
      "Episode 4: Loss=0.6283\n",
      "Episode 4: Loss=0.6470\n",
      "Episode 4: Loss=0.6728\n",
      "Episode 4: Loss=0.6937\n",
      "Episode 4: Loss=0.7193\n",
      "Episode 4: Loss=0.7968\n",
      "Episode 4: Loss=0.8471\n",
      "Episode 4: Loss=0.8802\n",
      "Episode 4: Loss=0.9082\n",
      "Episode 4: Loss=0.9335\n",
      "Episode 4: Loss=0.9755\n",
      "Episode 4: Loss=1.0435\n",
      "Episode 4: Loss=1.0631\n",
      "Episode 4: Loss=1.0854\n",
      "Episode 4: Loss=1.0899\n",
      "Episode 4: Loss=1.0974\n",
      "Episode 4: Loss=1.2078\n",
      "Episode 4: Loss=1.2424\n",
      "Episode 4: Loss=1.2435\n",
      "Episode 4: Loss=1.2697\n",
      "Episode 4: Loss=1.3555\n",
      "Episode 4: Loss=1.3880\n",
      "Episode 4: Loss=1.4166\n",
      "Episode 4: Loss=1.5008\n",
      "Episode 4: Loss=1.5851\n",
      "Episode 4: Loss=1.7012\n",
      "Episode 4: Loss=1.7496\n",
      "Episode 4: Loss=1.7585\n",
      "Episode 4: Loss=1.7692\n",
      "Episode 4: Loss=1.7802\n",
      "Episode 4: Loss=1.7916\n",
      "Episode 4: Loss=1.8232\n",
      "Episode 4: Loss=1.8938\n",
      "Episode 4: Loss=1.9266\n",
      "Episode 5: Loss=0.0245\n",
      "Episode 5: Loss=0.0320\n",
      "Episode 5: Loss=0.0841\n",
      "Episode 5: Loss=0.1060\n",
      "Episode 5: Loss=0.1226\n",
      "Episode 5: Loss=0.1542\n",
      "Episode 5: Loss=0.2363\n",
      "Episode 5: Loss=0.2621\n",
      "Episode 5: Loss=0.2836\n",
      "Episode 5: Loss=0.3325\n",
      "Episode 5: Loss=0.3762\n",
      "Episode 5: Loss=0.4135\n",
      "Episode 5: Loss=0.4555\n",
      "Episode 5: Loss=0.4645\n",
      "Episode 5: Loss=0.5365\n",
      "Episode 5: Loss=0.5554\n",
      "Episode 5: Loss=0.5699\n",
      "Episode 5: Loss=0.5906\n",
      "Episode 5: Loss=0.6018\n",
      "Episode 5: Loss=0.6306\n",
      "Episode 5: Loss=0.6471\n",
      "Episode 5: Loss=0.6833\n",
      "Episode 5: Loss=0.7056\n",
      "Episode 5: Loss=0.7353\n",
      "Episode 5: Loss=0.7385\n",
      "Episode 5: Loss=0.7653\n",
      "Episode 5: Loss=0.7813\n",
      "Episode 5: Loss=0.8094\n",
      "Episode 5: Loss=0.8522\n",
      "Episode 5: Loss=0.8752\n",
      "Episode 5: Loss=0.9019\n",
      "Episode 5: Loss=0.9768\n",
      "Episode 5: Loss=1.0076\n",
      "Episode 5: Loss=1.0151\n",
      "Episode 5: Loss=1.2195\n",
      "Episode 5: Loss=1.2687\n",
      "Episode 5: Loss=1.3007\n",
      "Episode 5: Loss=1.3056\n",
      "Episode 5: Loss=1.3306\n",
      "Episode 5: Loss=1.4785\n",
      "Episode 5: Loss=1.4943\n",
      "Episode 5: Loss=1.4975\n",
      "Episode 5: Loss=1.5248\n",
      "Episode 5: Loss=1.5458\n",
      "Episode 5: Loss=1.5620\n",
      "Episode 5: Loss=1.5838\n",
      "Episode 5: Loss=1.6294\n",
      "Episode 5: Loss=1.6446\n",
      "Episode 5: Loss=1.6671\n",
      "Episode 5: Loss=1.6868\n",
      "Episode 6: Loss=0.0147\n",
      "Episode 6: Loss=0.0768\n",
      "Episode 6: Loss=0.0928\n",
      "Episode 6: Loss=0.1265\n",
      "Episode 6: Loss=0.1350\n",
      "Episode 6: Loss=0.1462\n",
      "Episode 6: Loss=0.1518\n",
      "Episode 6: Loss=0.1998\n",
      "Episode 6: Loss=0.2625\n",
      "Episode 6: Loss=0.2913\n",
      "Episode 6: Loss=0.3175\n",
      "Episode 6: Loss=0.3266\n",
      "Episode 6: Loss=0.3426\n",
      "Episode 6: Loss=0.4113\n",
      "Episode 6: Loss=0.4310\n",
      "Episode 6: Loss=0.4404\n",
      "Episode 6: Loss=0.4653\n",
      "Episode 6: Loss=0.4660\n",
      "Episode 6: Loss=0.4808\n",
      "Episode 6: Loss=0.5022\n",
      "Episode 6: Loss=0.5066\n",
      "Episode 6: Loss=0.5073\n",
      "Episode 6: Loss=0.5221\n",
      "Episode 6: Loss=0.5301\n",
      "Episode 6: Loss=0.5446\n",
      "Episode 6: Loss=0.5975\n",
      "Episode 6: Loss=0.6186\n",
      "Episode 6: Loss=0.6280\n",
      "Episode 6: Loss=0.6721\n",
      "Episode 6: Loss=0.6780\n",
      "Episode 6: Loss=0.6877\n",
      "Episode 6: Loss=0.7431\n",
      "Episode 6: Loss=0.7506\n",
      "Episode 6: Loss=0.7999\n",
      "Episode 6: Loss=0.8692\n",
      "Episode 6: Loss=0.9058\n",
      "Episode 6: Loss=0.9295\n",
      "Episode 6: Loss=0.9405\n",
      "Episode 6: Loss=1.0042\n",
      "Episode 6: Loss=1.0330\n",
      "Episode 6: Loss=1.0545\n",
      "Episode 6: Loss=1.1381\n",
      "Episode 6: Loss=1.2106\n",
      "Episode 6: Loss=1.2889\n",
      "Episode 6: Loss=1.3231\n",
      "Episode 6: Loss=1.3392\n",
      "Episode 6: Loss=1.3511\n",
      "Episode 6: Loss=1.4127\n",
      "Episode 6: Loss=1.4365\n",
      "Episode 6: Loss=1.4533\n",
      "Episode 7: Loss=0.0096\n",
      "Episode 7: Loss=0.0203\n",
      "Episode 7: Loss=0.1034\n",
      "Episode 7: Loss=0.1261\n",
      "Episode 7: Loss=0.1397\n",
      "Episode 7: Loss=0.1999\n",
      "Episode 7: Loss=0.2233\n",
      "Episode 7: Loss=0.2835\n",
      "Episode 7: Loss=0.3084\n",
      "Episode 7: Loss=0.3180\n",
      "Episode 7: Loss=0.3251\n",
      "Episode 7: Loss=0.3867\n",
      "Episode 7: Loss=0.4330\n",
      "Episode 7: Loss=0.4437\n",
      "Episode 7: Loss=0.4968\n",
      "Episode 7: Loss=0.5942\n",
      "Episode 7: Loss=0.6359\n",
      "Episode 7: Loss=0.6585\n",
      "Episode 7: Loss=0.6722\n",
      "Episode 7: Loss=0.8105\n",
      "Episode 7: Loss=0.8781\n",
      "Episode 7: Loss=0.9987\n",
      "Episode 7: Loss=1.0383\n",
      "Episode 7: Loss=1.0640\n",
      "Episode 7: Loss=1.1081\n",
      "Episode 7: Loss=1.1483\n",
      "Episode 7: Loss=1.1563\n",
      "Episode 7: Loss=1.1682\n",
      "Episode 7: Loss=1.1840\n",
      "Episode 7: Loss=1.2317\n",
      "Episode 7: Loss=1.2910\n",
      "Episode 7: Loss=1.3411\n",
      "Episode 7: Loss=1.3519\n",
      "Episode 7: Loss=1.3591\n",
      "Episode 7: Loss=1.3981\n",
      "Episode 7: Loss=1.4026\n",
      "Episode 7: Loss=1.4527\n",
      "Episode 7: Loss=1.4559\n",
      "Episode 7: Loss=1.4687\n",
      "Episode 7: Loss=1.5076\n",
      "Episode 7: Loss=1.5406\n",
      "Episode 7: Loss=1.6458\n",
      "Episode 7: Loss=1.6504\n",
      "Episode 7: Loss=1.6548\n",
      "Episode 7: Loss=1.7172\n",
      "Episode 7: Loss=1.7916\n",
      "Episode 7: Loss=1.8087\n",
      "Episode 7: Loss=1.8211\n",
      "Episode 7: Loss=1.8623\n",
      "Episode 7: Loss=1.9012\n",
      "Episode 8: Loss=0.0157\n",
      "Episode 8: Loss=0.0731\n",
      "Episode 8: Loss=0.1103\n",
      "Episode 8: Loss=0.1487\n",
      "Episode 8: Loss=0.1752\n",
      "Episode 8: Loss=0.1856\n",
      "Episode 8: Loss=0.2211\n",
      "Episode 8: Loss=0.2971\n",
      "Episode 8: Loss=0.3534\n",
      "Episode 8: Loss=0.4411\n",
      "Episode 8: Loss=0.4671\n",
      "Episode 8: Loss=0.4736\n",
      "Episode 8: Loss=0.4753\n",
      "Episode 8: Loss=0.4828\n",
      "Episode 8: Loss=0.4933\n",
      "Episode 8: Loss=0.5157\n",
      "Episode 8: Loss=0.5275\n",
      "Episode 8: Loss=0.5721\n",
      "Episode 8: Loss=0.5971\n",
      "Episode 8: Loss=0.6331\n",
      "Episode 8: Loss=0.6367\n",
      "Episode 8: Loss=0.7170\n",
      "Episode 8: Loss=0.7213\n",
      "Episode 8: Loss=0.7340\n",
      "Episode 8: Loss=0.7514\n",
      "Episode 8: Loss=0.7780\n",
      "Episode 8: Loss=0.7874\n",
      "Episode 8: Loss=0.7891\n",
      "Episode 8: Loss=0.8018\n",
      "Episode 8: Loss=0.8815\n",
      "Episode 8: Loss=0.8880\n",
      "Episode 8: Loss=0.8957\n",
      "Episode 8: Loss=0.9813\n",
      "Episode 8: Loss=1.0297\n",
      "Episode 8: Loss=1.0331\n",
      "Episode 8: Loss=1.0423\n",
      "Episode 8: Loss=1.0545\n",
      "Episode 8: Loss=1.0819\n",
      "Episode 8: Loss=1.0850\n",
      "Episode 8: Loss=1.1433\n",
      "Episode 8: Loss=1.1704\n",
      "Episode 8: Loss=1.2101\n",
      "Episode 8: Loss=1.2150\n",
      "Episode 8: Loss=1.3000\n",
      "Episode 8: Loss=1.3117\n",
      "Episode 8: Loss=1.3454\n",
      "Episode 8: Loss=1.3649\n",
      "Episode 8: Loss=1.4355\n",
      "Episode 8: Loss=1.4457\n",
      "Episode 8: Loss=1.4524\n",
      "Episode 9: Loss=0.0133\n",
      "Episode 9: Loss=0.0601\n",
      "Episode 9: Loss=0.1132\n",
      "Episode 9: Loss=0.1562\n",
      "Episode 9: Loss=0.1681\n",
      "Episode 9: Loss=0.1699\n",
      "Episode 9: Loss=0.2176\n",
      "Episode 9: Loss=0.2247\n",
      "Episode 9: Loss=0.2691\n",
      "Episode 9: Loss=0.2762\n",
      "Episode 9: Loss=0.3713\n",
      "Episode 9: Loss=0.3852\n",
      "Episode 9: Loss=0.4709\n",
      "Episode 9: Loss=0.5275\n",
      "Episode 9: Loss=0.6312\n",
      "Episode 9: Loss=0.6424\n",
      "Episode 9: Loss=0.6595\n",
      "Episode 9: Loss=0.7175\n",
      "Episode 9: Loss=0.7271\n",
      "Episode 9: Loss=0.7471\n",
      "Episode 9: Loss=0.7865\n",
      "Episode 9: Loss=0.8045\n",
      "Episode 9: Loss=0.8251\n",
      "Episode 9: Loss=0.8466\n",
      "Episode 9: Loss=0.8839\n",
      "Episode 9: Loss=0.9196\n",
      "Episode 9: Loss=0.9848\n",
      "Episode 9: Loss=0.9932\n",
      "Episode 9: Loss=1.0346\n",
      "Episode 9: Loss=1.0545\n",
      "Episode 9: Loss=1.0621\n",
      "Episode 9: Loss=1.0766\n",
      "Episode 9: Loss=1.0934\n",
      "Episode 9: Loss=1.1630\n",
      "Episode 9: Loss=1.1666\n",
      "Episode 9: Loss=1.1759\n",
      "Episode 9: Loss=1.1779\n",
      "Episode 9: Loss=1.1893\n",
      "Episode 9: Loss=1.3643\n",
      "Episode 9: Loss=1.4149\n",
      "Episode 9: Loss=1.4367\n",
      "Episode 9: Loss=1.4426\n",
      "Episode 9: Loss=1.4697\n",
      "Episode 9: Loss=1.4897\n",
      "Episode 9: Loss=1.4973\n",
      "Episode 9: Loss=1.5045\n",
      "Episode 9: Loss=1.5100\n",
      "Episode 9: Loss=1.5522\n",
      "Episode 9: Loss=1.5687\n",
      "Episode 9: Loss=1.5721\n",
      "Episode 10: Loss=0.0063\n",
      "Episode 10: Loss=0.1245\n",
      "Episode 10: Loss=0.1396\n",
      "Episode 10: Loss=0.1487\n",
      "Episode 10: Loss=0.2076\n",
      "Episode 10: Loss=0.2446\n",
      "Episode 10: Loss=0.2608\n",
      "Episode 10: Loss=0.2614\n",
      "Episode 10: Loss=0.2984\n",
      "Episode 10: Loss=0.3079\n",
      "Episode 10: Loss=0.3122\n",
      "Episode 10: Loss=0.3311\n",
      "Episode 10: Loss=0.3356\n",
      "Episode 10: Loss=0.3764\n",
      "Episode 10: Loss=0.3829\n",
      "Episode 10: Loss=0.3989\n",
      "Episode 10: Loss=0.4301\n",
      "Episode 10: Loss=0.4515\n",
      "Episode 10: Loss=0.4585\n",
      "Episode 10: Loss=0.4657\n",
      "Episode 10: Loss=0.5016\n",
      "Episode 10: Loss=0.5785\n",
      "Episode 10: Loss=0.6627\n",
      "Episode 10: Loss=0.7193\n",
      "Episode 10: Loss=0.7246\n",
      "Episode 10: Loss=0.7406\n",
      "Episode 10: Loss=0.7562\n",
      "Episode 10: Loss=0.8449\n",
      "Episode 10: Loss=0.8564\n",
      "Episode 10: Loss=0.8807\n",
      "Episode 10: Loss=0.8839\n",
      "Episode 10: Loss=0.9001\n",
      "Episode 10: Loss=0.9108\n",
      "Episode 10: Loss=0.9420\n",
      "Episode 10: Loss=0.9563\n",
      "Episode 10: Loss=0.9972\n",
      "Episode 10: Loss=1.0361\n",
      "Episode 10: Loss=1.0737\n",
      "Episode 10: Loss=1.1353\n",
      "Episode 10: Loss=1.2002\n",
      "Episode 10: Loss=1.2433\n",
      "Episode 10: Loss=1.2519\n",
      "Episode 10: Loss=1.2752\n",
      "Episode 10: Loss=1.3369\n",
      "Episode 10: Loss=1.3811\n",
      "Episode 10: Loss=1.4385\n",
      "Episode 10: Loss=1.4508\n",
      "Episode 10: Loss=1.5213\n",
      "Episode 10: Loss=1.5396\n",
      "Episode 10: Loss=1.6160\n",
      "Training Finished. The agent is ready\n",
      "Analyzing new candidate\n",
      "Openness: 0.29\n",
      "Conscientiousness: 0.29\n",
      "Extroversion: 0.51\n",
      "Agreeableness: 0.33\n",
      "Neuroticism: 0.31\n",
      "Model saved to hiring_agent_model.pth\n"
     ]
    }
   ],
   "source": [
    "class BERTEncoder:\n",
    "    '''\n",
    "    The 'Eyes' of the AI. Reads text and converts to numbers.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        print('Loading BERT Model')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', max_length= 128, truncation=True, padding='max_length')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # Return [CLS] token embedding (size : 768)\n",
    "        return outputs.last_hidden_state[:, 0, : ]\n",
    "\n",
    "class HiringAgent(nn.Module):\n",
    "    '''\n",
    "    The 'Brain' of the AI.\n",
    "    Inputs: BERT Vector (768) + Meta Features (3) = 771 Inputs\n",
    "    Output: 5 Personality Scores\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(HiringAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(771, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, bert_vec, meta_vec):\n",
    "        combined = torch.cat((bert_vec, meta_vec), dim= 1)\n",
    "        x = self.fc1(combined)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class TrainingSession:\n",
    "    def __init__(self):\n",
    "        self.bert = BERTEncoder()\n",
    "        self.agent = HiringAgent()\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr= LEARNING_RATE)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def load_data(self):\n",
    "        if not os.path.exists(DATA_FILE):\n",
    "            print(f\"Error: {DATA_FILE} not found\")\n",
    "            return []\n",
    "        with open(DATA_FILE, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded {len(data)} labeled resumes for training\")\n",
    "        return data\n",
    "\n",
    "    def train(self):\n",
    "        data = self.load_data()\n",
    "        if not data: return\n",
    "        print('Starting AI Training')\n",
    "\n",
    "        for episode in range(EPISODES):\n",
    "            random.shuffle(data)\n",
    "            total_loss= 0\n",
    "\n",
    "            # Simple Batch Processing\n",
    "            for item in data[:50]:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # 1. Prepare Inputs\n",
    "                text_vec = self.bert.encode(item['text'])\n",
    "\n",
    "                # Normalize meta features [Length, Bullets, Caps]\n",
    "                # We divide length by 1000 to keep numbers small (Neural Nets like small numbers)\n",
    "                meta_raw = item['meta_features']\n",
    "                meta_norm = [meta_raw[0]/1000.0, meta_raw[1]/10.0, meta_raw[2]]\n",
    "                meta_vec = torch.tensor([meta_norm], dtype= torch.float32)\n",
    "\n",
    "                # 2. AI Prediction\n",
    "                prediction = self.agent(text_vec, meta_vec)\n",
    "\n",
    "                # 3. The \"Correct Answer\" (from Gemini)\n",
    "                target = torch.tensor([item['labels']], dtype=torch.float32)\n",
    "\n",
    "                # 4. Learning (Backpropagation)\n",
    "                loss = self.loss_fn(prediction, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                print(f\"Episode {episode+1}: Loss={total_loss:.4f}\")\n",
    "        print('Training Finished. The agent is ready')\n",
    "    def predict_new_candidate(self, resume_text):\n",
    "        '''\n",
    "        Applying the trained model on a brand new resume\n",
    "        '''\n",
    "        print('Analyzing new candidate')\n",
    "        t_vec = self.bert.encode(resume_text)\n",
    "        meta = [len(resume_text)/1000.0, resume_text.count('‚Ä¢')/10.0, 0.1]\n",
    "        m_vec = torch.tensor([meta], dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = self.agent(t_vec, m_vec)[0]\n",
    "        traits = [\"Openness\", \"Conscientiousness\", \"Extroversion\", \"Agreeableness\", \"Neuroticism\"]\n",
    "        for t, s in zip(traits, scores):\n",
    "            print(f\"{t}: {s:.2f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    session = TrainingSession()\n",
    "    session.train()\n",
    "    sample_resume = \"Highly organized software engineer. Led a team of 10. Loves public speaking.\"\n",
    "    session.predict_new_candidate(sample_resume)\n",
    "    torch.save(session.agent.state_dict(), 'hiring_agent_model.pth')\n",
    "    print('Model saved to hiring_agent_model.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a30d9a-ec28-46eb-9f03-2cd31977784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run : streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01204ef5-72d1-4a5c-a0a8-a114109c0931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9df578-47dc-4655-9678-231438de4d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b571fd8-35a4-4838-9bf2-530f160d963f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
