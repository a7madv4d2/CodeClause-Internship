# âœ‹ Real-Time Hand Gesture Recognition System

A robust **Humanâ€“Computer Interaction (HCI)** system that uses **Deep Learning** and **Computer Vision** to interpret hand gestures in real time.  
The project features a **custom VGG-style Convolutional Neural Network (CNN)** and a professional **Heads-Up Display (HUD)** interface for controlling virtual environments or devices without physical touch.

---

## ğŸš€ Key Features

### ğŸ§  Deep Learning Brain
- Custom **VGG-style CNN architecture**
- Includes **Batch Normalization** and **Dropout**
- Robust classification of **10 distinct hand gestures**

### âš¡ Real-Time Inference
- Low-latency prediction pipeline
- **OpenCV** for frame processing
- **TensorFlow** for real-time inference

### ğŸ›ï¸ Professional HUD Interface
- ğŸ“Š **Live Probability Analytics**  
  Sidebar visualization showing confidence scores for all gesture classes
- ğŸ¯ **Dynamic ROI**  
  Bounding box color changes based on prediction confidence
- ğŸšï¸ **Sensitivity Tuner**  
  Real-time slider to adjust binary thresholding and handle lighting variations

### ğŸ–ï¸ Functional Gesture Mapping
- 10 gestures mapped to simulated system commands  
  *(e.g., volume control, media navigation)*

---

## ğŸ› ï¸ Tech Stack

- **Language:** Python
- **Computer Vision:** OpenCV (`cv2`)
- **Deep Learning:** TensorFlow / Keras
- **Data Manipulation:** NumPy, Shutil :contentReference[oaicite:1]{index=1}

---

## ğŸ“‚ Dataset Source

This project was trained on the **LeapGestRecog â€“ Hand Gesture Recognition Database**.

- **Source:** Kaggle â€“ Hand Gesture Recognition Database
- **Content:**  
  20,000 Infrared (IR) images  
  10 gestures Ã— 10 subjects

**Important Note:**  
Although the dataset contains **IR images**, this project applies **data augmentation** and **adaptive thresholding** to generalize effectively to standard **RGB webcam feeds**. 
---

## ğŸ® Gesture Controls

The system recognizes the following gestures and maps them to actions:

| Gesture | Action | Symbol |
|------|-------|-------|
| Palm | Pause Video | âœ‹ |
| Fist | Grab / Hold | âœŠ |
| Thumb Up | Volume Up / Like | ğŸ‘ |
| Thumb Down | Volume Down / Dislike | ğŸ‘ |
| Index Point | Click / Select | â˜ï¸ |
| OK Sign | Confirm Selection | ğŸ‘Œ |
| Palm Moved | Swipe Screen | ğŸ‘‹ |
| Fist Moved | Drag Object | âœŠ |
| C-Sign | Copy | ğŸ‡¨ |
| L-Sign | Lock Screen | ğŸ‡± |

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/a7madv4d2/Hand-Gestures-Recognition-System.git
cd Hand-Gestures-Recognition-System
```

### 2ï¸âƒ£ Install Dependencies
```
pip install opencv-python tensorflow numpy matplotlib
```

## â–¶ï¸ Running the Pipeline

The project follows a **Jupyter Notebookâ€“based workflow**:

### 1ï¸âƒ£ Organize Data
Run the data organization script to flatten the dataset structure into clean class folders.

### 2ï¸âƒ£ Train Model
Run the training notebook or script to generate the trained model file: gesture_model_robust.h5

### 3ï¸âƒ£ Run Application
Execute the application script to launch the webcam-based hand gesture recognition interface.



