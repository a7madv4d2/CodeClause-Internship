{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43cc704e-e18b-407a-aee5-9d5f1257b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fed747-8529-436f-8782-87db65b2b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = 'leapGestRecog'\n",
    "OUTPUT_PATH = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5962199e-246d-4375-894b-722beb1144e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GESTURES_TO_KEEP = {\n",
    "    \"01_palm\": \"palm\",\n",
    "    \"02_l\": \"l_sign\",\n",
    "    \"03_fist\": \"fist\",\n",
    "    \"04_fist_moved\": \"fist_moved\",\n",
    "    \"05_thumb\": \"thumb\",\n",
    "    \"06_index\": \"index\",\n",
    "    \"07_ok\": \"ok\",\n",
    "    \"08_palm_moved\": \"palm_moved\",\n",
    "    \"09_c\": \"c_sign\",\n",
    "    \"10_down\": \"down\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7bdaa2-0fbc-4d86-aa1f-43450f9e4657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all 10 gestures...\n",
      " Success! Organized 20000 images (All 10 Classes) into 'data/'.\n"
     ]
    }
   ],
   "source": [
    "# --- DATA ORGANIZER (ROBUST VERSION) ---\n",
    "if not os.path.exists(RAW_DATA_PATH):\n",
    "    print(f\" Error: Could not find '{RAW_DATA_PATH}'\")\n",
    "else:\n",
    "    # Clear old data to ensure a fresh start with all 10 classes\n",
    "    if os.path.exists(OUTPUT_PATH):\n",
    "        shutil.rmtree(OUTPUT_PATH)\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "    count = 0\n",
    "    print(\"Processing all 10 gestures...\")\n",
    "    \n",
    "    for subject_folder in os.listdir(RAW_DATA_PATH):\n",
    "        subject_path = os.path.join(RAW_DATA_PATH, subject_folder)\n",
    "        if not os.path.isdir(subject_path): continue\n",
    "\n",
    "        for gesture_folder in os.listdir(subject_path):\n",
    "            # strict check to ensure we only get the folders we defined\n",
    "            if gesture_folder in GESTURES_TO_KEEP:\n",
    "                new_name = GESTURES_TO_KEEP[gesture_folder]\n",
    "                target_folder = os.path.join(OUTPUT_PATH, new_name)\n",
    "                \n",
    "                if not os.path.exists(target_folder):\n",
    "                    os.makedirs(target_folder)\n",
    "\n",
    "                src_gesture_path = os.path.join(subject_path, gesture_folder)\n",
    "                \n",
    "                for image_name in os.listdir(src_gesture_path):\n",
    "                    if not image_name.lower().endswith(('.png', '.jpg', '.jpeg')): continue\n",
    "                        \n",
    "                    src_image = os.path.join(src_gesture_path, image_name)\n",
    "                    new_image_name = f\"subject{subject_folder}_{image_name}\"\n",
    "                    dst_image = os.path.join(target_folder, new_image_name)\n",
    "                    \n",
    "                    shutil.copy(src_image, dst_image)\n",
    "                    count += 1\n",
    "\n",
    "    print(f\" Success! Organized {count} images (All 10 Classes) into '{OUTPUT_PATH}/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da696cd-3fb0-4b57-8b5e-640bf01af274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 10 classes.\n",
      "Found 4000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "\n",
    "# Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 15,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0.1,\n",
    "    zoom_range = 0.1,\n",
    "    brightness_range = [0.8, 1.2],\n",
    "    horizontal_flip = False,\n",
    "    fill_mode = 'nearest',\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "# Loading training data with advanced augmentation\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    OUTPUT_PATH,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    OUTPUT_PATH,\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "class_indices = training_set.class_indices\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b778fb4-377b-429b-be50-03390257cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep CNN Architecture (VGG-Style)\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3348aee1-b440-40f0-a2f2-be8599aa9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: low-level features (Edges, Lines)\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=(64,64,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4f39eff-c801-46f3-97ac-d17fe53b3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 2: Mid-level features (Shapes, Curves)\n",
    "model.add(Conv2D(64, (3,3), padding= 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f08e77b2-0055-454a-bea1-9fc9ebc44734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block3: High_level features(complex gestures)\n",
    "model.add(Conv2D(128, (3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27210e95-7185-42bb-81cf-5b79dafd4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten & Dense\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=len(class_indices), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cd2edc7-b37f-420b-80d3-4526d8bf5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22d4c36a-c8ef-4dc2-8796-89416688f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 1s/step - accuracy: 0.8022 - loss: 0.5979 - val_accuracy: 0.4895 - val_loss: 2.0596\n",
      "Epoch 2/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 416ms/step - accuracy: 0.9732 - loss: 0.0884 - val_accuracy: 0.6630 - val_loss: 1.6682\n",
      "Epoch 3/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 419ms/step - accuracy: 0.9833 - loss: 0.0524 - val_accuracy: 0.9190 - val_loss: 0.2773\n",
      "Epoch 4/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 420ms/step - accuracy: 0.9886 - loss: 0.0379 - val_accuracy: 0.4790 - val_loss: 3.5955\n",
      "Epoch 5/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 416ms/step - accuracy: 0.9899 - loss: 0.0336 - val_accuracy: 0.9402 - val_loss: 0.2475\n",
      "Epoch 6/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 418ms/step - accuracy: 0.9906 - loss: 0.0307 - val_accuracy: 0.9240 - val_loss: 0.2129\n",
      "Epoch 7/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 417ms/step - accuracy: 0.9918 - loss: 0.0269 - val_accuracy: 0.7670 - val_loss: 1.0201\n",
      "Epoch 8/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 419ms/step - accuracy: 0.9910 - loss: 0.0257 - val_accuracy: 0.9545 - val_loss: 0.1832\n",
      "Epoch 9/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 408ms/step - accuracy: 0.9937 - loss: 0.0207 - val_accuracy: 0.8590 - val_loss: 0.7727\n",
      "Epoch 10/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 407ms/step - accuracy: 0.9942 - loss: 0.0177 - val_accuracy: 0.9118 - val_loss: 0.2774\n",
      "Epoch 11/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 412ms/step - accuracy: 0.9941 - loss: 0.0186 - val_accuracy: 0.8595 - val_loss: 0.5354\n",
      "Epoch 12/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 409ms/step - accuracy: 0.9942 - loss: 0.0200 - val_accuracy: 0.9778 - val_loss: 0.0727\n",
      "Epoch 13/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 410ms/step - accuracy: 0.9973 - loss: 0.0102 - val_accuracy: 0.9227 - val_loss: 0.3153\n",
      "Epoch 14/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 430ms/step - accuracy: 0.9953 - loss: 0.0150 - val_accuracy: 0.9215 - val_loss: 0.2938\n",
      "Epoch 15/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 446ms/step - accuracy: 0.9952 - loss: 0.0153 - val_accuracy: 0.9712 - val_loss: 0.0962\n",
      "Epoch 16/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 476ms/step - accuracy: 0.9953 - loss: 0.0147 - val_accuracy: 0.9578 - val_loss: 0.1618\n",
      "Epoch 17/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 452ms/step - accuracy: 0.9959 - loss: 0.0143 - val_accuracy: 0.9143 - val_loss: 0.4809\n",
      "Epoch 18/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 404ms/step - accuracy: 0.9956 - loss: 0.0144 - val_accuracy: 0.9567 - val_loss: 0.1419\n",
      "Epoch 19/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 399ms/step - accuracy: 0.9973 - loss: 0.0084 - val_accuracy: 0.8528 - val_loss: 0.6146\n",
      "Epoch 20/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 353ms/step - accuracy: 0.9966 - loss: 0.0118 - val_accuracy: 0.9140 - val_loss: 0.6485\n",
      "Epoch 21/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 340ms/step - accuracy: 0.9976 - loss: 0.0075 - val_accuracy: 0.9165 - val_loss: 0.3450\n",
      "Epoch 22/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 346ms/step - accuracy: 0.9964 - loss: 0.0126 - val_accuracy: 0.9513 - val_loss: 0.2416\n",
      "Epoch 23/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 344ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 0.8198 - val_loss: 1.0899\n",
      "Epoch 24/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 310ms/step - accuracy: 0.9964 - loss: 0.0112 - val_accuracy: 0.9778 - val_loss: 0.1012\n",
      "Epoch 25/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 312ms/step - accuracy: 0.9983 - loss: 0.0052 - val_accuracy: 0.9693 - val_loss: 0.1428\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(training_set, validation_data=validation_set, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03471f5d-4043-4c4d-9e80-2b91c3d0b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('gesture_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1539e0a-6a14-445b-b3cb-687fdbab85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gesture_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66db645e-b433-4dcb-ae88-86a99eccacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'gesture_model.keras'\n",
    "JSON_PATH = 'class_indices.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2eb69cf-a08b-4062-a436-88cd84c46234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 32 variables whereas the saved optimizer has 62 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "#loading the model\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c1c3de7-8acf-41f6-96a7-ca9faeafdfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load class names ( 0 : 'fist', etc)\n",
    "with open(JSON_PATH, 'r') as f:\n",
    "    class_indices = json.load(f)\n",
    "gesture_map = {v: k for k, v in class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6140a4c-6653-4178-80b1-09a0545e48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ACTION CONTROLLER ---\n",
    "def perform_action(gesture_name):\n",
    "    \"\"\"\n",
    "    Translates the recognized gesture into a command.\n",
    "    \"\"\"\n",
    "    gesture_name = gesture_name.lower()\n",
    "    \n",
    "    # Mapping 10 gestures to actions\n",
    "    if \"palm\" in gesture_name and \"moved\" not in gesture_name:\n",
    "        return \"PAUSE MEDIA\"\n",
    "    elif \"palm_moved\" in gesture_name:\n",
    "        return \"SWIPE / WAVE\"\n",
    "    elif \"fist\" in gesture_name and \"moved\" not in gesture_name:\n",
    "        return \"GRAB / HOLD\"\n",
    "    elif \"fist_moved\" in gesture_name:\n",
    "        return \"DRAG OBJECT\"\n",
    "    elif \"index\" in gesture_name:\n",
    "        return \"CLICK / SELECT\"\n",
    "    elif \"ok\" in gesture_name:\n",
    "        return \"CONFIRM\"\n",
    "    elif \"thumb\" in gesture_name:\n",
    "        return \"LIKE / VOL UP\"\n",
    "    elif \"down\" in gesture_name:\n",
    "        return \"DISLIKE / VOL DOWN\"\n",
    "    elif \"c\" in gesture_name:\n",
    "        return \"COPY\"\n",
    "    elif \"l\" in gesture_name:\n",
    "        return \"LOSE / LOCK\"\n",
    "        \n",
    "    return \"Waiting...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b2c7d2-00ac-4b5a-a75c-7a989a1d23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    '''\n",
    "    1.Crop ROI\n",
    "    2.Greyscale\n",
    "    3.Blur\n",
    "    4.Threshold\n",
    "    '''\n",
    "    # ROI (top left y: bottom right y, top left x: bottom right x)\n",
    "    roi = frame[100:300, 100:300]\n",
    "\n",
    "    #convert to greyscale\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # blur to remove camera noise\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 2)\n",
    "\n",
    "    # Binary thresholding - background is black, hand is white ( if background is light)\n",
    "    _, thresh = cv2.threshold(blur, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5910a9c-8ba4-4fed-84ea-816405331e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Camera on.\n",
      "Put your hand in the blue box\n",
      "Press 'ESC' to quit\n"
     ]
    }
   ],
   "source": [
    "# If you want to run directly\n",
    "# Main Loop\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print('\\n Camera on.')\n",
    "print('Put your hand in the blue box')\n",
    "print(\"Press 'ESC' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Error reading camera')\n",
    "        break\n",
    "    # Flip frame for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    #1 Get the processed image\n",
    "    processed_roi = preprocess_frame(frame)\n",
    "\n",
    "    #2 Prapare for the model\n",
    "    # Resize to 64*64 (model input size)\n",
    "    roi_resized = cv2.resize(processed_roi, (64, 64))\n",
    "    # Reshape to (1, 64, 64, 1) : (Batch, Height, Width, Channels)\n",
    "    final_input = roi_resized.reshape(1, 64, 64, 1)\n",
    "    # Normalize pixel values(0-1)\n",
    "    final_input = final_input / 255.0\n",
    "\n",
    "    #3 Predict\n",
    "    prediction = model.predict(final_input, verbose=0)\n",
    "    class_index = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "\n",
    "    # Get the name \n",
    "    gesture_name = gesture_map[class_index]\n",
    "\n",
    "    #4 Display logic\n",
    "    display_color = (0, 255, 0) # Green\n",
    "    action_text = '...'\n",
    "\n",
    "    # Only show prediction if confident > 70% \n",
    "    if confidence > 0.7:\n",
    "        action_text = perform_action(gesture_name)\n",
    "        # Show text\n",
    "        cv2.putText(frame, f\"Gesture: {gesture_name.upper()} ({int(confidence*100)}%)\", \n",
    "                    (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, display_color, 2)\n",
    "        cv2.putText(frame, f\"Action: {action_text}\", \n",
    "                    (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, \"Trying to detect...\", (10, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Draw ROI box on the main frame \n",
    "    cv2.rectangle(frame, (100, 100), (300, 300), (255, 0,0), 2)\n",
    "\n",
    "    # Show the windows\n",
    "    cv2.imshow('Hand Gesture Control', frame)\n",
    "    # Show what the AI sees\n",
    "    cv2.imshow('AI Vision (Binary Mask)', processed_roi)\n",
    "\n",
    "    # Exit condition\n",
    "    if cv2.waitKey(1) & 0xFF == 27: # ESC key\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd9f50b-c5c6-42f6-9ccb-f8bba0daed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " System Loaded.\n",
      "ðŸ“· Camera ON. Press 'ESC' to exit.\n"
     ]
    }
   ],
   "source": [
    "# If you want to try with UI\n",
    "# Press ESC to exit, don't close the window directly\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "#MODEL_PATH = 'gesture_model_robust.h5'\n",
    "#JSON_PATH = 'class_indices.json'\n",
    "\n",
    "# --- UI CLASS (Best Practice: Separate UI logic from App logic) ---\n",
    "class ProfessionalUI:\n",
    "    def __init__(self, class_names):\n",
    "        self.class_names = class_names\n",
    "        self.colors = [(0, 255, 0), (0, 165, 255), (0, 0, 255), (255, 255, 0), (255, 0, 255)]\n",
    "        \n",
    "    def draw_sidebar(self, frame, predictions, threshold_val):\n",
    "        \"\"\"Draws a semi-transparent sidebar with probability bars.\"\"\"\n",
    "        height, width, _ = frame.shape\n",
    "        sidebar_w = 250\n",
    "        \n",
    "        # 1. Create Overlay\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (0, 0), (sidebar_w, height), (30, 30, 30), -1)\n",
    "        \n",
    "        # 2. Apply Transparency (Alpha Blending)\n",
    "        alpha = 0.7\n",
    "        cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "        \n",
    "        # 3. Draw Header\n",
    "        cv2.putText(frame, \"AI ANALYTICS\", (20, 40), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.line(frame, (20, 50), (230, 50), (255, 255, 255), 1)\n",
    "\n",
    "        # 4. Draw Threshold Info\n",
    "        cv2.putText(frame, f\"Threshold: {threshold_val}\", (20, 80), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
    "\n",
    "        # 5. Draw Probability Bars\n",
    "        start_y = 120\n",
    "        for i, score in enumerate(predictions):\n",
    "            label = self.class_names[i].upper()\n",
    "            bar_len = int(score * 150) # Scale bar length\n",
    "            \n",
    "            # Highlight the winner\n",
    "            color = (100, 100, 100) # Gray\n",
    "            if score == max(predictions):\n",
    "                color = (0, 255, 0) # Green\n",
    "                \n",
    "            # Text\n",
    "            cv2.putText(frame, f\"{label}\", (20, start_y), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            # Bar Background\n",
    "            cv2.rectangle(frame, (100, start_y-10), (230, start_y), (50, 50, 50), -1)\n",
    "            # Bar Fill\n",
    "            cv2.rectangle(frame, (100, start_y-10), (100 + bar_len, start_y), color, -1)\n",
    "            # Percentage\n",
    "            cv2.putText(frame, f\"{int(score*100)}%\", (235, start_y), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)\n",
    "            \n",
    "            start_y += 35\n",
    "\n",
    "    def draw_hud_roi(self, frame, x, y, w, h, confidence):\n",
    "        \"\"\"Draws a futuristic corner-bracket ROI.\"\"\"\n",
    "        color = (0, 0, 255) # Red (Low Confidence)\n",
    "        if confidence > 0.8: \n",
    "            color = (0, 255, 0) # Green (High Confidence)\n",
    "        \n",
    "        # Draw corners instead of full box\n",
    "        line_len = 20\n",
    "        thick = 2\n",
    "        \n",
    "        # Top-Left\n",
    "        cv2.line(frame, (x, y), (x + line_len, y), color, thick)\n",
    "        cv2.line(frame, (x, y), (x, y + line_len), color, thick)\n",
    "        # Top-Right\n",
    "        cv2.line(frame, (x+w, y), (x+w - line_len, y), color, thick)\n",
    "        cv2.line(frame, (x+w, y), (x+w, y + line_len), color, thick)\n",
    "        # Bottom-Left\n",
    "        cv2.line(frame, (x, y+h), (x + line_len, y+h), color, thick)\n",
    "        cv2.line(frame, (x, y+h), (x, y+h - line_len), color, thick)\n",
    "        # Bottom-Right\n",
    "        cv2.line(frame, (x+w, y+h), (x+w - line_len, y+h), color, thick)\n",
    "        cv2.line(frame, (x+w, y+h), (x+w, y+h - line_len), color, thick)\n",
    "        \n",
    "        # Center Crosshair\n",
    "        cx, cy = x + w//2, y + h//2\n",
    "        cv2.line(frame, (cx-5, cy), (cx+5, cy), color, 1)\n",
    "        cv2.line(frame, (cx, cy-5), (cx, cy+5), color, 1)\n",
    "\n",
    "# --- LOAD RESOURCES ---\n",
    "try:\n",
    "    model = load_model(MODEL_PATH)\n",
    "    with open(JSON_PATH, 'r') as f:\n",
    "        class_indices = json.load(f)\n",
    "    # Get list of names in correct order\n",
    "    class_names = [k for k, v in sorted(class_indices.items(), key=lambda item: item[1])]\n",
    "    ui = ProfessionalUI(class_names)\n",
    "    print(\" System Loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "def nothing(x): pass\n",
    "\n",
    "window_name = \"Professional Gesture Control\"\n",
    "cv2.namedWindow(window_name)\n",
    "cv2.createTrackbar(\"Sensitivity\", window_name, 127, 255, nothing)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"ðŸ“· Camera ON. Press 'ESC' to exit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # --- SAFETY CHECK: If user clicked 'X', stop the loop ---\n",
    "    try:\n",
    "        # Check if window is still open. If not, break the loop.\n",
    "        if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\n",
    "            \n",
    "        # 1. Get Slider Value\n",
    "        thresh_val = cv2.getTrackbarPos(\"Sensitivity\", window_name)\n",
    "    except:\n",
    "        # If any GUI error happens, stop safely\n",
    "        break\n",
    "    \n",
    "    # 2. Preprocessing\n",
    "    roi_x, roi_y, roi_w, roi_h = 350, 100, 250, 250\n",
    "    roi = frame[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w]\n",
    "    \n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 2)\n",
    "    _, thresh = cv2.threshold(blur, thresh_val, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # 3. Prediction\n",
    "    final_input = cv2.resize(thresh, (64, 64))\n",
    "    final_input = final_input.reshape(1, 64, 64, 1) / 255.0\n",
    "    \n",
    "    prediction = model.predict(final_input, verbose=0)[0]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    # 4. Draw UI\n",
    "    ui.draw_sidebar(frame, prediction, thresh_val)\n",
    "    ui.draw_hud_roi(frame, roi_x, roi_y, roi_w, roi_h, confidence)\n",
    "    \n",
    "    cv2.imshow(\"AI Vision\", thresh)\n",
    "    cv2.imshow(window_name, frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27: # ESC key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be16aaa-6c40-4299-a689-0ed341466ff7",
   "metadata": {},
   "source": [
    "While the model achieved 96% accuracy on the validation set, real-world deployment showed sensitivity to lighting conditions. This is due to the Domain Gap between the training data (clean Infrared images) and the input data (Standard Webcam RGB converted to Binary). The model struggles to generalize when the thresholding artifacting differs from the clean edges of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5534dc5-b2ca-4cb5-942a-1f179c647ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
